---
title: "Fuel Economy Forecast"
output: html_document
---

Ok, so. This may be a futile exercise. But, we just really need SOME idea of how MPGs have evolved for light, heavy, and medium fleet vehicles over the past several years. I just need a BAU estimate for the fleet action plan. Of course, I will exponenitally decay this function such that the cars do not get supremely efficient. However, what we currrently have is not sufficient in my view, because it is not founded in anything. We have absolutely no idea how much more efficienct vehicles might actually get in the aggregate. 

By forecasting with aggregate statistics, we avoid violating simple rules for statistical inference. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidypredict)
library(readxl)
library(tidyquant)
library(tidyverse)
library(magrittr)
library(curl)

library(gridExtra)
library(scales)
library(readxl)
library(plotly)
library(ggthemes)
library(forecast)

```

## R data loading and manip

```{r ggtheme}
theme_Publication <- function(base_size=14, base_family="helvetica") {
      library(grid)
      library(ggthemes)
      (theme_foundation(base_size=base_size, base_family=base_family)
       + theme(plot.title = element_text(face = "bold",
                                         size = rel(1.2), hjust = 0.5),
               text = element_text(),
               panel.background = element_rect(colour = NA),
               plot.background = element_rect(colour = NA),
               panel.border = element_rect(colour = NA),
               axis.title = element_text(face = "bold",size = rel(1)),
               axis.title.y = element_text(angle=90,vjust =2),
               axis.title.x = element_text(vjust = -0.2),
               axis.text = element_text(), 
               axis.line = element_line(colour="black"),
               axis.ticks = element_line(),
               panel.grid.major = element_line(colour="#f0f0f0"),
               panel.grid.minor = element_blank(),
               legend.key = element_rect(colour = NA),
               legend.position = "bottom",
               legend.direction = "horizontal",
               legend.key.size= unit(0.2, "cm"),
               legend.margin = unit(0, "cm"),
               legend.title = element_text(face="italic"),
               plot.margin=unit(c(10,5,5,5),"mm"),
               strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
               strip.text = element_text(face="bold")
          ))
      
}

scale_fill_Publication <- function(...){
      library(scales)
      discrete_scale("fill","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)

}

scale_colour_Publication <- function(...){
      library(scales)
      discrete_scale("colour","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)

}
```


```{r cars}
url<-"https://www.fueleconomy.gov/feg/epadata/vehicles.csv.zip"
tmp<-tempfile()
download.file(url,dest=tmp)
data<-read_csv(unz(tmp,"vehicles.csv"))
data2<-read_excel("vehmpg.xlsx",sheet="vehmpg")



```

## summarizations
So, it looks like all of these vehilces are under 8,500 lbs. 

```{r EPA veh dataset}

facs<-c("VClass","year")   #the factor levels that I think I want to work with. 

vars<-c("city08U","cityA08U","comb08U","combA08U","highway08U","highwayA08U")

sums<-data %>% group_by(VClass,year) %>% summarise_at(vars,mean,na.rm=T) %>% ggplot(aes(x=year,y=highway08U,color=VClass)) + geom_point() + scale_fill_Publication() +theme_Publication()

```

```{r epa Trends dataset}

gg1<-data2[data2$type != "All",] %>% ggplot(aes(x=year,y=comb.mpg,color=type)) + geom_line() + ylab("Combined Est. Real-World MPG") + xlab("Year") + labs(fill = "Type of Car")
gg1<-gg1 + labs(color="Type of Car")  #Has to be color to fill up. 
gg1 %>% ggplotly()

data2<-data2 %>% group_by(type) %>% mutate(comb.lag = comb.mpg - lag(comb.mpg)) %>% mutate(comb.lagpct= (comb.mpg - lag(comb.mpg))/lag(comb.mpg)) %>% ungroup()


gg2<-data2[data2$type != "All",] %>% ggplot(aes(x=year,y=comb.lagpct,color=type)) + geom_line() + ylab("Delta Combined Est. Real-World MPG") + xlab("Year") + labs(fill = "Type of Car")
gg2<-gg2 + labs(color="Type of Car")  #Has to be color to fill up. 
gg2 %>% ggplotly()

gg3<-data2[data2$type == "All",] %>% group_by(type,year) %>% summarize_at("comb.lagpct",mean) %>% ggplot(aes(x=year,y=comb.lagpct)) + geom_line() + ylab("Delta Combined Est. Real-World MPG") + xlab("Year") + labs(fill = "Type of Car")
 #Has to be color to fill up. 
gg3 %>% ggplotly()




```


So, from just a visual plot of the percent differenced curves, we should exclude data before 1990 or so. With the trend data now, we have enough to make a simple forecast curve for the next 10 - 15 years or so, I think. The non-constant variance will make a broader model quite difficult methinks. So, let's just make a basic forecast of the time series. This will be a BAU therefore for just the light passenger fleet. 

Quickly, you learn that after differencing, there is like zero lag. This is great news, as simulating the series will be much more straight forward. This is basically a random-walk function. So, I am going to simply fit an empirical PDF and simulate that baby 10,000 times to get probabilistic estimates. 

#Time series model
```{r many models}
library(MASS)
library(ecdfHT)
library(randomForest)


dat2<-data2 %>% select("comb.lagpct","type","year") %>% filter(year>1989 & type == "All") %>% group_by(year) %>% summarize_at("comb.lagpct",mean)


a<-ecdfHT(dat, scale.q = c(.25,.5,.75), show.axes.labels = TRUE,
show.plot = TRUE, type = "l")

#Cannot nest the ECD objects unfortunately. Oh well. 
#Data must be a vector

mods<-ts(dat2$comb.lagpct,start=1990,end=2018)

arima_sim<-function(nsim,data,h){
       #a<- ecdfHT(data, scale.q = c(.25,.5,.75), show.axes.labels = TRUE,
       #show.plot = FALSE, type = "l")
       # fit <-  ecdfHT.fit(c(.1,.9), a, add.to.plot=FALSE,x.max=.2)
       fit<-auto.arima(data)
       lis<-list()
       for(i in 1:nsim){
           # x<-recdfHT(h, fit) %>% tibble() %>% mutate(year=seq(2019,length.out = h)) %>% mutate(run=i)
            x<-simulate(fit,h,future=T,bootstrap=T)   %>% as.vector() %>% tibble() %>% mutate(year=seq(2019,length.out = h)) %>%   mutate(run=i)
           names(x)<-c(paste0("output"),"year","run")
          lis[[i]]<-x
   
       }
        afn<-bind_rows(lis) #%>% gather(runs)   #This works pretty good now.
            return(afn)
      }

test<-arima_sim(nsim=1000,data=mods,h=20)






p<-seq(0.1,1,0.1)
p_names <- map_chr(p, ~paste0(.x*100, "%"))  
p_funs <- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %>% 
  set_names(nm = p_names)

results<-test %>% 
  group_by(year) %>% 
  summarize_at(vars(output), funs(!!!p_funs)) %>% gather(`deciles`,value,-year)


results3<-test %>% 
  group_by(year) %>% 
  summarize_at(vars(output),mean) %>% write.csv(.,"mpg.sim.res.csv")



results2<-test %>% group_by(year) %>% 
  summarize_at(vars(output),median)

gg5<-results %>% ggplot(aes(x=year,y=value,color=deciles)) + geom_line(alpha=1/3)

gg5 %>% ggplotly()


```

## Analysis of actual fleet data  


The analysis of the national data is interesting. It's a reasonable forecast. But it's only reasonable for making broad generalizations about the stock of US fleet, assuming that light US fleet's composition stay's exactly the same as is. 

For the most part, we should only assume BAU reductions for the light fleet, probably gas only. 

```{r fleet mpg}
fmpg<-read_excel("fleetmpg.xlsx")

#fmpg<- 
  
fmpg<-fmpg %>% dplyr::filter(SEGMENT != "Off", mpg > 0, mpg < 60,  zpg < 2.5, zpg > -2.5) %>% group_by(SEGMENT,YEAR,FUEL_TYPE) %>% summarize_at(c("mpg","MILES_TRAVELED"),mean,na.rm=T)

gg6<-test %>% ggplot(aes(x=YEAR,y=mpg,color=SEGMENT,size=MILES_TRAVELED)) + geom_point()


fit<-lm(log(mpg)~YEAR*FUEL_TYPE + SEGMENT + FUEL_TYPE,data=fmpg)
fit2<-lm(log(mpg)~YEAR*FUEL_TYPE + SEGMENT + FUEL_TYPE,data=fmpg)   #statistical evidence for an interaction term. 

fmpg$SEGMENT %<>% as.factor()
fmpg$FUEL_TYPE %<>% as.factor()

newdata2<-seq(2018,2030) %>% enframe() %>% select(value) %>% mutate(YEAR=as.factor(value)) %>% select(YEAR)

newdata2<-expand.grid(newdata2$YEAR,levels(fmpg$SEGMENT),levels(fmpg$FUEL_TYPE)) %>% rename(YEAR=Var1,SEGMENT=Var2,FUEL_TYPE=Var3) 
newdata2$YEAR %<>% as.character %>% as.numeric()

fcast<-predict(fit2,newdata2)

newdata2$preds<-exp(fcast)
newdata2 %<>% group_by(FUEL_TYPE,SEGMENT) %>% mutate(mpg.lagpct = (preds - lag(preds))/lag(preds)) %>% ungroup()
write.csv(newdata2,"mpg_trend.csv")

```

